{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install binary into system path\n",
    "echo \"PATH=/usr/local/bin:$PATH\" >> ~/.bash_profile && \\\n",
    "source ~/.bash_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowpal Wabbit\n",
    "VW (Vowpal Wabbit) is a flexible tool for online and batch learning of (mostly) linear models. It has been designed with efficiency and simplicity as its core principles. It is not a complete description of VW, but every effort has been made to have pointers to more information throughout the document.\n",
    "\n",
    "These notes are what I think are important after reading [Vowpal Wabbit tutorial for the Uninitiated](https://www.zinkov.com/posts/2013-08-13-vowpal-tutorial/), the [wiki](https://github.com/JohnLangford/vowpal_wabbit/wiki) and Yahoo's Scalable Machine Learning Wiki\n",
    "\n",
    "## Installation on Mac\n",
    "```shell\n",
    "# install the boost library\n",
    "brew install boost\n",
    "\n",
    "# get vw\n",
    "git clone git://github.com/JohnLangford/vowpal_wabbit.git\n",
    "\n",
    "# change directory\n",
    "cd vowpal_wabbit\n",
    "\n",
    "# install\n",
    "make\n",
    "\n",
    "# test\n",
    "make test\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Input Format\n",
    "\n",
    "    [key \\t] label [importance] [base] [tag]|namespace feature ... |namespace feature ... \\n\n",
    "where namespace = `string[:float]` and feature = `string[:float]`\n",
    "\n",
    "example:\n",
    "```\n",
    "1.0 1.0 w1;1.0;1.0;W6K6lkoG7v5SHMYjTWrTwQbhrV.h5E3Dcj8ADTnv|a gpos3ttfc:0.218977 gpos6ttc gpos8ctr:0.275933  |q int2_spans_count:0.255002 max_df_localdcat:0.307283 min_idf_webf1:0.351295 int2_class_place_name vikings int3_spans_count:0.130118 max_idf_webf1:0.283068 max_idf_webf2:0.262892 nwords:0.231262 avg_idf_webf2:0.249664 minnesota qnav:0.496886\n",
    "```\n",
    "\n",
    "Here, the label is 1.0, the importance weight is 1.0, the tag w1;1.0;1.0;W6K6lkoG7v5SHMYjTWrTwQbhrV.h5E3Dcj8ADTnv has assorted meta-data such as the page-view ID of the page corresponding to these feature, and the example has two namespaces: a and q. Note that VW admits sparse representations and string feature names: in essence, the input format is little more that groups of key-value pairs, where the value can be omitted if it's equal to 1 and the whole pair can be omitted if the value is zero (thus, implicitly, a missing feature is assumed to have a value of zero).\n",
    "\n",
    "Tags are important for passing identifier and meta-data to the predictions. In the absence of a tag, only the output of the model (the score) will be written. Namespaces are useful for feature grouping and such feature grouping comes in handy when constructing quadratic pairs.\n",
    "An optional but important piece of an example is tab-separated \"key\" at the beginning: this key is completely ignored by VW, but is useful for data generation with Hadoop streaming, as during the reduce phase the outputs will be sorted by this key. Typical values are a random integer (to shuffle the data) or the time-stamp of the page-view (to have the data in sequential order).\n",
    "\n",
    "Note that the tag must be adjacent to the namespace separator '|' without extra whitespace in-between. Similarly namespace must follow '|' without any whitespace.\n",
    "\n",
    "## Training\n",
    "```\n",
    "zcat train.vw.gz | vw --cache_file train.cache -f data.model --compressed\n",
    "```\n",
    "`--cache-file arg` is where the data is stored in a format easier for vw to reuse. `-f arg` specifies the filename of the output model/predictor. By default none is created. `--compressed` will make it a point to try to process the data and store caches and models in a gzip-compressed format.\n",
    "\n",
    "## Validation and Testing\n",
    "```\n",
    "zcat test.vw.gz | vw -t --cache_file test.cache -i data.model -p test.pred\n",
    "```\n",
    "the `-t` option tells vw to ignore the labels and not train on the data. The `-i arg` option specifies the model. The `-p arg` option specifies the output file of the prediction.\n",
    "\n",
    "## Model Inspection\n",
    "If you want a human readable model, use `--readable_model arg` instead of `-f arg`. To perserve the feature names instead of seeing hashes, use `--invert_hash arg`\n",
    "\n",
    "## Example - Houston Housing Price\n",
    "We will predict housing prices in Houston\n",
    "\n",
    "```\n",
    "vw boston.data.vw --readable_model --invert_hash boston.model\n",
    "```\n",
    "`housing_model`:\n",
    "```\n",
    "Version 7.3.0\n",
    "Min label:0.000000\n",
    "Max label:50.000000\n",
    "bits:18\n",
    "0 pairs:\n",
    "0 triples:\n",
    "rank:0\n",
    "lda:0\n",
    "0 ngram:\n",
    "0 skip:\n",
    "options:\n",
    ":0\n",
    "^AGE:0.013058\n",
    "^B:0.013684\n",
    "^CHAS:3.058681\n",
    "^CRIM:-0.047248\n",
    "^DIS:0.385468\n",
    "^INDUS:-0.052709\n",
    "^LSTAT:-0.165589\n",
    "^NOX:3.014200\n",
    "^PTRATIO:0.124905\n",
    "^RAD:-0.072961\n",
    "^RM:0.713633\n",
    "^TAX:-0.000079\n",
    "^ZN:0.054472\n",
    "Constant:4.484257\n",
    "```\n",
    "```\n",
    "^AGE:104042:28.8:0.0188122@1.18149e+09\n",
    "```\n",
    "^AGE is the feature name, 104042 is the hashed value for the feature, 28.8 is the value of that feature for this example, 0.0188122 is the weight we have learned thus far for the feature, and 1.1849e9 is a sum of gradients squared over the feature values, which is used for adjusting the weight on the feature.\n",
    "\n",
    "## Turning the learning algorithm\n",
    "By default, vw minimizaes the following function:\n",
    "<img src='./img/minfunction.png' />\n",
    "Where λ1 and λ2 refer to L1 and L2 regularization functions. ℓ refers to the loss function.\n",
    "\n",
    "`--l1 arg` and `--l2 arg` controls l1 and l2 regularization lambda values\n",
    "\n",
    "`--loss_function arg` default to `squared`, but can be `logistic`, `hinge`, `quantile`, `poisson`\n",
    "\n",
    "`-l arg` [`--learning_rate arg`] (=0.5) sets the initial learning rate\n",
    "\n",
    "`--decay_learning_rate arg` (=1) sets Decay factor for learning_rate between passes\n",
    "\n",
    "`--feature_mask arg` is usually used with `l1` to learn which features to include\n",
    "```shell\n",
    "vw -d data.vw --l1 0.001 -f features.model\n",
    "vw -d data.vw -i features.model --feature_mask features.model -f final.model\n",
    "```\n",
    "\n",
    "`--keep arg` keep namespaces beginning with character arg\n",
    "\n",
    "`--ignore arg` ignore namespaces beginning with character arg\n",
    "\n",
    "`--adaptive` (on by default) use adaptive learning rate $\\alpha$ for each parameter in the model \n",
    "\n",
    "`--normalized` (on by default) use per feature normalized updates\n",
    "\n",
    "`--invariant` (on by default) \n",
    "\n",
    "## Multiclass\n",
    "### One vs all\n",
    "`--oaa` only requires that the labels are between 1-k (inclusive) classes\n",
    "\n",
    "## Nonlinear (Quadratic and Cubic)\n",
    "`--quadratic arg` [`-q arg`] creates quadratic features between 2 namespaces beginning with characters in arg (limitation: only first letters of the namespace can be used)\n",
    "\n",
    "`--cubic arg` creates cubic features between 3 namespaces beginning with characters in arg (limitation: only first letters of the namespace can be used)\n",
    "\n",
    "## Active Learning\n",
    "for unlabelled data, vw can ask for labels whenever it thinks it needs it. The labeller can be a human, or a process on a certain port.\n",
    "\n",
    "```shell\n",
    "cat data.vw | vw --active_simulation --active_mellowness 0.000001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
