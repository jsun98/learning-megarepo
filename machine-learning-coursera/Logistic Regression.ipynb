{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "A binary classification algorithm (output between 0 - 1), called a 'regression' because of historical reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "$$\n",
    "h_\\theta(x) = g(\\theta^Tx)\n",
    "$$\n",
    "$h_\\theta(x) means $ probability that y = 1 given x, or $h_\\theta(x) = P(y = 1 | x ; \\theta)$\n",
    "\n",
    "### Sigmoid function / Logistic function\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "<img src='./img/3.1.png' width = 400px />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "Suppose we have $h_\\theta(x) = g(\\theta^Tx)$, and we determine that if $h_\\theta(x) \\ge$ 0.5, then y = 1. Since z = $\\theta^Tx$, this is equivalent to $\\theta^Tx \\ge 0$. So the decision boundary equation is $\\theta_0 + \\theta_1x_1 + ... + \\theta_nx_n \\ge 0$ or $\\theta_1x_1 + ... + \\theta_nx_n \\ge -\\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^m Cost(h_\\theta(x), y)\n",
    "$$\n",
    "$$\n",
    "Cost(h_\\theta(x), y) = \n",
    "\\begin{cases} \n",
    "      -log(h_\\theta(x)) & if \\space y = 1 \\\\\n",
    "      -log(1 - h_\\theta(x)) & if \\space y = 0 \\\\\n",
    "   \\end{cases}\n",
    "\\\n",
    "$$\n",
    "More concisely: \n",
    "$$\n",
    "Cost(h_\\theta(x), y) = -y \\times log(h_\\theta(x)) - (1 - y) \\times log(1 - h_\\theta(x))\n",
    "$$\n",
    "Therefore:\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m y \\times log(h_\\theta(x)) + (1 - y) \\times log(1 - h_\\theta(x))\n",
    "$$\n",
    "\n",
    "If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.\n",
    "\n",
    "If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.\n",
    "\n",
    "<img src='./img/3.2.png' />\n",
    "<img src='./img/3.3.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent for Logistic Regression\n",
    "Goal: min($J(\\theta)$)\n",
    "\n",
    "algorithm:\n",
    "\n",
    "repeat (simultaneous update) {\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)}) \\times x_j^{(i)}$\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
