{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "## Terminology\n",
    "**Activation Function**: sigmoid/logistic function, denoted $a_i^{(j)}$ - activation unit of unit i in layer j\n",
    "\n",
    "**Weights**: parameters $\\theta$\n",
    "\n",
    "**Input layer**: layer 1, where we input our examples\n",
    "\n",
    "**Hidden layer**: layers other than input and output layer\n",
    "\n",
    "**Output layer**: last layer, outputs our hypothesis\n",
    "\n",
    "**$\\Theta^{(j)}$**: matrix of weights controlling function mapping from layer j to layer j + 1\n",
    "\n",
    "\n",
    "## Overview\n",
    "<img src='img/4.1.png' />\n",
    "\n",
    "if network has $s_j$ units in layer j, $s_{j+1}$ units in layer j+ 1, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$\n",
    "\n",
    "each row is corrosponds to one neurons in the l+1 layer excluding the bias unit. Each column corrosponds to one neuron in the l'th layer including the bias unit.\n",
    "\n",
    "## Forward Propagation Vectorization\n",
    "$$\n",
    "z^{(j+1)} = \\Theta^{(j)}a^{(j)} \\\\\n",
    "a^{(j+1)} = g(z^{(j+1)}) \\\\\n",
    "a_0^{(j+1)} = 1\n",
    "$$\n",
    "note: $a^{(1)} = x$\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) \\in R^K \\\\\n",
    "y \\in R^K\n",
    "$$\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m}[\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}log(h_\\Theta(x^i))_k + (1-y_k^i)log(1-(h_\\Theta(x^i))_k] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_i}\\sum_{j=1}^{j_i+1}(\\Theta_{ji}^l)^2\n",
    "$$\n",
    "where m is the number of training examples\n",
    "\n",
    "L is the number of layers\n",
    "\n",
    "K is the number of neurons in the output layer\n",
    "\n",
    "$s_l$ is the number of neurons in layer l\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "$\\delta_j^{l}$ is the error of node j in layer l\n",
    "\n",
    "example:\n",
    "$$\n",
    "\\delta^{(4)} = a^{(4)} - y \\\\\n",
    "\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)} .* g'(z^{(3)})\n",
    "$$\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "set $\\Delta_{ij}^{(l)} = 0$ for all l,i,j\n",
    "\n",
    "for i = 1 to m:\n",
    "\n",
    "set $a^{(1)} = x^{(i)}$\n",
    "\n",
    "perform forward propagation to compute $a^{(l)}$ for ll=2,3,...,L\n",
    "\n",
    "using $y^{(i)}$, compute $\\delta^{(L)}=a^{(L)}-y^{(i)}$\n",
    "\n",
    "compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ...\\delta^{(2)}$\n",
    "\n",
    "## Training a neural network\n",
    "number of input layer neurons: dimension of the features\n",
    "number of output layer neurons: number of classes\n",
    "hidden layers: more the better, but too much can be very computationally expensive\n",
    "\n",
    "1. randomly initizlize weights\n",
    "2. implement forward propagation to compute the hypothesis\n",
    "3. implement code to compute the loss function\n",
    "4. implement back propagation to compute the partial derivatives\n",
    "5. use gradient checking to compare $\\frac{\\alpha}{\\alpha\\Theta_{jk}^l}J(\\Theta)$ computed using backpropagation vs. using numerical estimate of graident. Then dis\n",
    "6. use an optimization method to minimize $J(\\Theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
