{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "## Terminology\n",
    "**Activation Function**: sigmoid/logistic function, denoted $a_i^{(j)}$ - activation unit of unit i in layer j\n",
    "\n",
    "**Weights**: parameters $\\theta$\n",
    "\n",
    "**Input layer**: layer 1, where we input our examples\n",
    "\n",
    "**Hidden layer**: layers other than input and output layer\n",
    "\n",
    "**Output layer**: last layer, outputs our hypothesis\n",
    "\n",
    "**$\\Theta^{(j)}$**: matrix of weights controlling function mapping from layer j to layer j + 1\n",
    "\n",
    "\n",
    "## Overview\n",
    "<img src='img/4.1.png' />\n",
    "\n",
    "if network has $s_j$ units in layer j, $s_{j+1}$ units in layer j+ 1, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$\n",
    "\n",
    "this is because $s_{j+1}$ are the rows each of which corrosponds to the parameters of $s_{j+1}$ units in layer j, and $(s_j + 1)$ are the columns each of which is the parameters of the units in layer j and the +1 includes the `bias unit`.\n",
    "\n",
    "## Forward Propagation Vectorization\n",
    "$$\n",
    "z^{(j+1)} = \\Theta^{(j)}a^{(j)} \\\\\n",
    "a^{(j+1)} = g(z^{(j+1)}) \\\\\n",
    "a_0^{(j+1)} = 1\n",
    "$$\n",
    "note: $a^{(1)} = x$\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "$$\n",
    "h_\\Theta(x) \\in R^K\n",
    "$$\n",
    "$$\n",
    "J(\\Theta) = -\\frac{1}{m}[\\sum_{i=1}^m\\sum_{k=1}^Ky_k^{(i)}log(h_\\Theta(x^i))_k + (1-y_k^i)log(1-(h_\\Theta(x^i))_k] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_i}\\sum_{j=1}^{j_i+1}(\\Theta_{ji}^l)^2\n",
    "$$\n",
    "where m is the number of training examples\n",
    "\n",
    "L is the number of layers\n",
    "\n",
    "K is the number of neurons in the output layer\n",
    "\n",
    "$s_l$ is the number of neurons in layer l\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "$\\delta_j^{l}$ is the error of node j in layer l\n",
    "\n",
    "example:\n",
    "$$\n",
    "\\delta^{(4)} = a^{(4)} - y \\\\\n",
    "\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)} .* g'(z^{(3)})\n",
    "$$\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "set $\\Delta_{ij}^{(l)} = 0$ for all l,i,j\n",
    "\n",
    "for i = 1 to m:\n",
    "\n",
    "set $a^{(1)} = x^{(i)}$\n",
    "\n",
    "perform forward propagation to compute $a^{(l)}$ for ll=2,3,...,L\n",
    "\n",
    "using $y^{(i)}$, compute $\\delta^{(L)}=a^{(L)}-y^{(i)}$\n",
    "\n",
    "compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ...\\delta^{(2)}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
