{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "[Stanford ppt](https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/trees.pdf)\n",
    "\n",
    "Decision trees involve a set of splitting rules used tosegment the predictor space. Algorithms that use decision trees include `bagging`, `random forests` and `boosting`. This method can be used for both regression and classification problems.\n",
    "\n",
    "## Terminology\n",
    "`terminal node`: the regions $R_t$ that predictions can fall into.\n",
    "\n",
    "`internal node`: the points along the tree where the predictor space is split\n",
    "\n",
    "## Regression Tree building\n",
    "1. divide the predictor space (aka the set of possible values for x) into J distinct and non-overlapping regions $R_1,R_2,...,R_j$\n",
    "2. For every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.\n",
    "\n",
    "the goal is to find 'boxes' $R_1,...,R_j$ that minimize the following loss function:\n",
    "\n",
    "$$\n",
    "\\sum^J_{j=1}\\sum_{i \\in R_j}(y_i - p_{R_j})^2\n",
    "$$\n",
    "\n",
    "where $p_{R_j}$ is the mean of the training observations in the jth box\n",
    "\n",
    "## Recusive Binary Split\n",
    "This is a top-down, greedy algorithm to split the predictor space in j boxes.\n",
    "\n",
    "It is top-down because it begins at the top of the tree and successively splits the predictor space; each split is indicated via two new branches further down on the tree. \n",
    "\n",
    "It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n",
    "\n",
    "### The algorithm\n",
    "1. Select the feature $X_j$ and the cutpoint s such that splitting the predictor space into the regions{X|$X_j$< s} and {X|$X_j$ ≥ s} leads to the greatest possible reduction in the loss function.\n",
    "2. Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions.\n",
    "3. Again, we look to split one of these three regions further, so as to minimize the loss function. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five examples.\n",
    "\n",
    "We predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\n",
    "\n",
    "### Overfitting and pruning\n",
    "The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance.\n",
    "\n",
    "We can grow a very large tree, then prune it back to obtain a subtree using `cost complexity pruning` or `weakest link pruning`.\n",
    "\n",
    "we consider a sequence of trees indexed by a nonnegative tuning parameter α. For each value of α there corresponds a subtree T ⊂ $T_0$ such that\n",
    "\n",
    "$$\n",
    "\\sum^{|T|}_{m=1}\\sum_{i,x_i \\in R_m}(y_i - p_{R_m})^2 + \\alpha|T|\n",
    "$$\n",
    "\n",
    "is as small as possible. Here |T| indicates the number of terminal nodes of the tree T, Rm is the rectangle (i.e. the subset of predictor space) corresponding to the mth terminal node, and yˆRm is the mean of the training observations in Rm.\n",
    "\n",
    "### Summary of the complete algorithm\n",
    "1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of α.\n",
    "3. Use K-fold cross-validation to choose α. For each k = 1,...,K:\n",
    "    - Repeat Steps 1 and 2 on the $\\frac{K-1}{K}$th fraction of the training data, excluding the Kth fold\n",
    "    - Evaluate the mean squared prediction error on the data in the left-out kth fold, as a function of α. \n",
    "    Average the results, and pick α to minimize the average error.\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of α.\n",
    " \n",
    "## Classification Tree Building\n",
    "Similar to regression trees, but this time, we use the classification error rate instead of the residual sum of squares.\n",
    "$$\n",
    "E = 1 - max_k(p_{mk})\n",
    "$$\n",
    "where $p_{mk}$ represents the proportion of training observations in the mth region that are from the kth class.\n",
    "\n",
    "this is simply the fraction of the training observations in that region that do not belong to the most common class\n",
    "\n",
    "but a better approach is to use the `Gini Index`:\n",
    "$$\n",
    "G = \\sum^K_{k=1}p_{mk}(1-p_{mk})\n",
    "$$\n",
    "G is said to be indicative of node `purity`. If variance is small, then G is small - node contains predominantly observations from a single class.\n",
    "\n",
    "another approach (which is often numerically equivelent to the Gini index) is `cross-entropy`:\n",
    "$$\n",
    "D = - \\sum^K_{k=1}p_{mk}log(p_{mk})\n",
    "$$\n",
    "\n",
    "## Pros and Cons of Trees\n",
    "Advantages:\n",
    "- Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n",
    "- Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.\n",
    "- Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\n",
    "- Trees can easily handle qualitative predictors without the need to create dummy variables.\n",
    "\n",
    "Disadvantage:\n",
    "- Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
