{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Hadoop, HDFS and Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "- Hadoop is a distributed file system\n",
    "- Efficient, automatic distribution of data and work across machines\n",
    "- High throughput access to information\n",
    "- Data is conceptually record-oriented in the Hadoop programming framework. The Hadoop framework then schedules these processes in proximity to the location of data/records using knowledge from the distributed file system. This strategy of moving computation to the data, instead of moving the data to the computation allows Hadoop to achieve high data locality which in turn results in high performance.\n",
    "![](https://farm4.static.flickr.com/3656/3529146359_3eb5d3e15b_o.png)\n",
    "- Hadoop programs must be written in the **MapReduce** model\n",
    "![](https://farm3.static.flickr.com/2344/3529959486_8f36fb28c5_o.png)\n",
    "- Flat scalability curve. Very little needs to be done to scale Hadoop to use larger amount of hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Distributed File System (HDFS) \n",
    "### Definitions\n",
    "**block**: Files may be broken into several blocks, default block size is 64MB\n",
    "**NameNode**: The NameNode stores all the metadata for the file system.\n",
    "**DataNode**: A machine in a cluster\n",
    "\n",
    "### Core Concepts\n",
    "![](https://farm3.static.flickr.com/2050/3529146393_5c2e2c8065_o.png)\n",
    "- HDFS expects to store very large files\n",
    "- Large block size reduces amount of metadata stored\n",
    "- HDFS files are **not visible** to the Linux operating system hosting Hadoop\n",
    "- data is replicated across several machines, system still works even if 1 or serveral machines fail\n",
    "\n",
    "### Advantages\n",
    "- HDFS is designed to store a very large amount of information (terabytes or petabytes). This requires spreading the data across a large number of machines. It also supports much larger file sizes than NFS.\n",
    "- HDFS should store data reliably. If individual machines in the cluster malfunction, data should still be available.\n",
    "- HDFS should provide fast, scalable access to this information. It should be possible to serve a larger number of clients by simply adding more machines to the cluster.\n",
    "- HDFS should integrate well with Hadoop MapReduce, allowing data to be read and computed upon locally when possible.\n",
    "\n",
    "### Disadvantages\n",
    "- Applications that use HDFS are assumed to perform long sequential streaming reads from files. HDFS is optimized to provide streaming read performance; this comes at the expense of random seek times to arbitrary positions in files.\n",
    "- Data will be written to the HDFS once and then read several times; updates to files after they have already been closed are not supported. (An extension to Hadoop will provide support for appending new data to the ends of files; it is scheduled to be included in Hadoop 0.19 but is not available yet.)\n",
    "- Due to the large size of files, and the sequential nature of reads, the system does not provide a mechanism for local caching of data. The overhead of caching is great enough that data should simply be re-read from HDFS source.\n",
    "- Individual machines are assumed to fail on a frequent basis, both permanently and intermittently. The cluster must be able to withstand the complete failure of several machines, possibly many happening at the same time (e.g., if a rack fails all together). While performance may degrade proportional to the number of machines lost, the system as a whole should not become overly slow, nor should information be lost. Data replication strategies combat this problem.\n",
    "\n",
    "### Example cluster setup\n",
    "`conf/hadoop-site.xml` - this file should be replicated across all machines\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.default.name</name>\n",
    "\n",
    "    <value>hdfs://your.server.name.com:9000</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.data.dir</name>\n",
    "\n",
    "    <value>/home/username/hdfs/data</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.name.dir</name>\n",
    "\n",
    "    <value>/home/username/hdfs/name</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "### Example user commands\n",
    "```shell\n",
    "# Hadoop Filesystem List command  \n",
    "bt> hadoop fs -ls  \n",
    "Found 3 items  \n",
    "drwxr-xr-x - bb8 users  0 2015-12-27 02:22 .Trash  \n",
    "drwxrwxrwx - bb8 users  0 2015-11-16 19:40 random_folder  \n",
    "drwxrwxrwx - bb8 users  0 2015-10-26 19:46 another_random_folder  \n",
    "# Note that the above command is identical to the one below, \"hadooop fs -ls\" without any parameters  \n",
    "# lists the content of the user's HDFS home directory  \n",
    "  \n",
    "bt> hadoop fs -ls /user/bb8  \n",
    "Found 3 items  \n",
    "drwxr-xr-x - bb8 users  0 2015-12-27 02:22 .Trash  \n",
    "drwxrwxrwx - bb8 users  0 2015-11-16 19:40 random_folder  \n",
    "drwxrwxrwx - bb8 users  0 2015-10-26 19:46 another_random_folder  \n",
    "  \n",
    "# Create a sample file in the local directory on the gateway under /homes/bb8  \n",
    "bt> pwd  \n",
    "/homes/bb8  \n",
    "bt> echo \"Hello World. My name is bb8. Hello\" > sample.txt  \n",
    "\n",
    "# Put (upload) the sample.txt file from local gateway to the HDFS  \n",
    "bt> hadoop fs -put sample.txt /user/bb8  \n",
    "bt> hadoop fs -ls /user/bb8  \n",
    "Found 4 items  \n",
    "drwxr-xr-x - bb8 users  0 2015-12-27 02:22 .Trash  \n",
    "drwxrwxrwx - bb8 users  0 2015-11-16 19:40 random_folder  \n",
    "drwxrwxrwx - bb8 users  0 2015-10-26 19:46 another_random_folder  \n",
    "-rw-------  3 bb8 users  29 2015-12-28 04:02 sample.txt  \n",
    "  \n",
    "# hadoop fs -cat /user/ilambharathi/sample.txt will also give the same output, whereas on  \n",
    "# compressed files (.gz, .bzip2), -text will give the uncompressed text as the output  \n",
    "bt> hadoop fs -text /user/bb8/sample.txt  \n",
    "Hello World. My name is bb8. Hello  \n",
    "  \n",
    "# Changing permissions on the file so that all can view the file.  \n",
    "bt> hadoop fs -chmod 755 /user/bb8/sample.txt  \n",
    "  \n",
    "# Check if the file permissions are changed  \n",
    "bt> hadoop fs -ls /user/bb8  \n",
    "Found 4 items  \n",
    "drwxr-xr-x - bb8 users  0 2015-12-27 02:22 .Trash  \n",
    "drwxrwxrwx - bb8 users  0 2015-11-16 19:40 random_folder  \n",
    "drwxrwxrwx - bb8 users  0 2015-10-26 19:46 another_random_folder  \n",
    "-rwxr-xr-x  3 bb8 users  29 2015-12-28 04:02 sample.txt  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "A programming paradigm that transforms lists of input data into lists of output data, twice - *map* and *reduce*. The MapReduce model uses functional programming concepts in that data is immutable (modifying input data will not propagate back to the original data). \n",
    "\n",
    "note: unlike some more formal functional mapping and reducing settings, in MapReduce, an arbitrary number of values can be output from each phase; a mapper may map one input into zero, one, or one hundred outputs. A reducer may compute over an input list and emit one or a dozen different outputs.\n",
    "\n",
    "### Map\n",
    "The first phase of a MapReduce program is called mapping. A list of data elements are provided, one at a time, to a function called the Mapper, which transforms each element individually to an output data element.\n",
    "![](https://farm3.static.flickr.com/2151/3529146569_f80eb39a44_o.png)\n",
    "\n",
    "### Reduce\n",
    "Reducing lets you aggregate values together. A reducer function receives an iterator of input values from an input list. It then combines these values together, returning a single output value.\n",
    "![](https://farm4.static.flickr.com/3213/3529959720_273aca53fe_o.png)\n",
    "\n",
    "### Putting them together in MapReduce\n",
    "**Keys and values**: In MapReduce, no value stands on its own. Every value has a key associated with it. Keys identify related values.\n",
    "\n",
    "### Example - Word Count\n",
    "High level structure:\n",
    "```Python\n",
    "mapper (filename, file-contents):\n",
    "  for each word in file-contents:\n",
    "    emit (word, 1)\n",
    "\n",
    "reducer (word, values):\n",
    "  sum = 0\n",
    "  for each value in values:\n",
    "    sum = sum + value\n",
    "  emit (word, sum)\n",
    "```\n",
    "\n",
    "\n",
    "Acutal (partial) implementation:\n",
    "```java\n",
    "public static class MapClass extends MapReduceBase\n",
    "    implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value,\n",
    "                    OutputCollector<Text, IntWritable> output,\n",
    "                    Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer itr = new StringTokenizer(line);\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * A reducer class that just emits the sum of the input values.\n",
    "   */\n",
    "  public static class Reduce extends MapReduceBase\n",
    "    implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "\n",
    "    public void reduce(Text key, Iterator<IntWritable> values,\n",
    "                       OutputCollector<Text, IntWritable> output,\n",
    "                       Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "### The Driver Method\n",
    "The driver initializes the job and instructs the Hadoop platform to execute your code on a set of input files, and controls where the output files are placed. \n",
    "\n",
    "```java\n",
    "public void run(String inputPath, String outputPath) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    // the keys are words (strings)\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    // the values are counts (ints)\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(MapClass.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    FileInputFormat.addInputPath(conf, new Path(inputPath));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(outputPath));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```\n",
    "\n",
    "### MapReduce Data Flow\n",
    "!()[https://farm4.static.flickr.com/3126/3529146657_5b5d025a5f_o.png]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
