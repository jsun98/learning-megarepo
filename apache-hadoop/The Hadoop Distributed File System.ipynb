{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hadoop Distributed File System\n",
    "When a dataset outgrows the storage capacity of a single physical machine, it becomes necessary to partition it across a number of separate machines. Filesystems that manage the storage across a network of machines are called *distributed filesystems*.\n",
    "\n",
    "## Design of the HDFS\n",
    "HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.\n",
    "\n",
    "`very large`: hundreds of terabytes of data\n",
    "\n",
    "`streaming`: write once, read many times. Each analysis will typically involve most, if not all, of the dataset\n",
    "\n",
    "`commodity hardware`: no expensive hardware required. However, node failure is commonplace, but HDFS is designed to carry on working without a noticeable interruption to the user in the face of such failure.\n",
    "\n",
    "**Drawbacks of the HDFS**: low lantency (ms) data access, storing many small files, multipler writers, random access\n",
    "\n",
    "## HDFS concepts\n",
    "`blocks`: A disk has a block size, which is the minimum amount of data that it can read or write. File systems store data in multiples of the block size. A typical HDFS block size is 128 MB. The HDFS block size is large because reading the block will be much longer than the time it takes to find a block head, thus transferring large files operate at disk transfer rate. Block abstraction brings serveral benefits: 1. a file can be larger than a disk as blocks from one file can be on multiple disks. 2. Fixed block sizes simpliy the storage subsystems. 3. it is good for replication and availibility. \n",
    "\n",
    "`namenode`: The namenode manages the filesystem namespace. It maintains the filesystem tree and the metadata for all the files and directories in the tree. This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log. The namenode also knows the datanodes on which all the blocks for a given file are located.\n",
    "\n",
    "in case of total namenode failure. There is no way to recover the files, since the mapping to reconstruct the file blocks is impossible to recover. Thus, we can either maintain a second backup namenode, or backup the metadata on a remote NFS.\n",
    "\n",
    "`datanode`: Datanodes are the workhorses of the filesystem. They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing.\n",
    "\n",
    "`block caching`: the namenode caches blocks that are frequently accessed.\n",
    "\n",
    "`HDFS federation`: \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
